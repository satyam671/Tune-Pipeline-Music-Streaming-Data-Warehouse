# Tune Pipeline - Music Streaming Data Warehouse
![Tune pipeline 2](https://github.com/user-attachments/assets/ae5f26a9-e39b-4f05-9f18-babd64c55fe3)

## Project Overview
A music streaming startup, Sparkify, has grown its user base and song database and wants to move its processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app and a directory with JSON metadata on the songs in their app.
The purpose of this project is to build an ETL pipeline that will be able to extract song data from an S3 bucket and transform that data to make it suitable for analysis. This data can be used with business intelligence and visualization apps that will help the analytics team to understand better what songs are commonly listened to on the app. In this project, we will create an ETL pipeline to build a data warehouse hosted on Redshift. 

## Dataset 
We will be working with two datasets ```song_data``` and ```log_data``` that reside in S3. 

#### Song Dataset: 
It's a subset of real data from [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

Sample Data:
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

#### Log Dataset:
The second dataset consists of log files in JSON format generated by this  [event simulator](https://github.com/Interana/eventsim)  based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset are partitioned by year and month. 

Sample Data: 

    {"artist":null,"auth":"Logged In","firstName":"Walter","gender":"M","itemInSession":0,"lastName":"Frye","length":null,"level":"free","location":"San Francisco-Oakland-Hayward, CA","method":"GET","page":"Home","registration":1540919166796.0,"sessionId":38,"song":null,"status":200,"ts":1541105830796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"39"}


## Database Schema Design
The project creates a redshift database in the cluster with staging tables that contain all the data retrieved from the s3 bucket and copied over to the tables. They are columnar which helps with parallelizing the execution of one query on multiple CPUs which makes it's performance much faster for large sets of data as compared to tables in relational databases. Each column on the tables corresponds to the keys in the JSON files and for the case of the staging_events table, since the column names were different from those in the file, I utilized a JSON path map file that maps the data elements to the relevant columns.

The project also creates a relational database with fact and dimension tables, therefore making it a star schema. The reason why I utilized the star schema as opposed to the 3rd normal form is because the star schema is more suitable and optimized for OLAP operations which will be the purpose of the database. This will store data from the staging tables that have been transformed to provide the relevant data in the tables.

## Schema for Song Play Analysis

#### Fact Table
songplays - records in event data associated with song plays. Columns for the table:

    songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimension Tables 
##### users

    user_id, first_name, last_name, gender, level
##### songs

    song_id, title, artist_id, year, duration

##### artists

    artist_id, name, location, lattitude, longitude

##### time

    start_time, hour, day, week, month, year, weekday

## Project Cloud Architechture
![diagram-export-24-8-2024-4_48_55-pm](https://github.com/user-attachments/assets/a67b9bb0-f8a6-429c-bc6b-c43fe62bfddf)

## ETL Pipeline
The data that gets extracted will need to be transformed to fit the data model in the target destination tables. For instance, the source data for timestamp is in unix format and that will need to be converted to timestamp from which the year, month, day, hour values etc can be extracted which will fit in the target database table schema.

Since the datasets, once copied over to the staging tables, will contain quite several columns, the query that inserts the data in the respective tables has a nested query that selects only the relevant columns for the table. In the case of the fact table, there is an added constraint for the data from each staging column that ensures that the values are not null.

![diagram-export-24-8-2024-4_56_38-pm](https://github.com/user-attachments/assets/6a60a143-66a8-4f26-8b38-462738558a39)

## Getting Started
In order to have a copy of the project up and running locally, you will need to take note of the following:
### Prerequisites
⨀ Python 2.7 or greater.

⨀ AWS Account

⨀ IAM role with AWS service as Redshift-Customizable and permissions set to s3 read only access.

⨀ Security group with inbound rules appropriately set as below:
```
Type: Custom TCP Rule.
Protocol: TCP.
Port Range: 5439,
Source: Custom IP, with 0.0.0.0/0
```
⨀ Set your AWS access and secret key in the config file.
```
[AWS]
KEY =<your aws key>
SECRET =<your aws secret>
```

### Installation
⨀ Make a new directory and clone/copy project files into it.

⨀ Create a virtualenv that will be your development environment, i.e:
```
$ virtualenv sparkify-project
$ source sparkify-project/bin/activate
```
⨀ Install the following packages in your virtual environment:
```
   - configparser
   - boto3
   - psycopg2
```
⨀ Alternatively you can install the requirements in the requirements.txt that's in this project by running the command:
```
$ pip install -r requirements.txt
```

## How to Run
#### Use [Redshift_Cluster_IaC.py](https://github.com/san089/Data_Engineering_Projects/blob/master/Redshift_Cluster_IaC.py "Redshift_Cluster_IaC.py") to launch Redshift Cluster.

#### Setup Configurations 
Setup the dwh.cfg file (File not added in this repository). File format for **dwh.cfg**

```
[CLUSTER]
HOST=''
DB_NAME=''
DB_USER=''
DB_PASSWORD=''
DB_PORT=5439

[IAM_ROLE]
ARN=<IAM Role arn>

[S3]
LOG_DATA='s3://"path_to"/log_data'
LOG_JSONPATH='s3://"path_to"/log_json_path.json'
SONG_DATA='s3://"path_to"/song_data'

```

#### Create tables

    $ python create_tables.py
## Screenshots of what the final tables look like
### Artists Table
![artists_table](https://github.com/user-attachments/assets/8a2d5da7-f93a-423d-ba96-094a37dd52b0)

### Song plays Table
![songplays_table_part1](https://github.com/user-attachments/assets/22c9e750-cc6c-4544-a44d-15d79f0db395)

![songplays_table_part2](https://github.com/user-attachments/assets/c35ba6c4-9c47-4d54-b41d-29281286d9cd)

### Songs Table

![songs_table](https://github.com/user-attachments/assets/3cace05e-d9c5-4339-86ff-f7427d0a1abd)

### Time Table

![time_table](https://github.com/user-attachments/assets/04411677-111f-445f-ad3e-e5e74fca6ff4)

### Users Table

![users_table](https://github.com/user-attachments/assets/3085db27-7baf-48b0-9fac-623179801cb6)


## Built With
⨀ Python and SQL

⨀ AWS S3 and Redshift


Reference: [AWS Redshift Doc](https://aws.amazon.com/redshift/getting-started/?p=rs&bttn=hero&exp=b)
